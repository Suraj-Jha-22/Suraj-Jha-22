## Hi there ðŸ‘‹ I'm Suraj Jha

âš¡ Data Scientist | ML Systems Engineer | GenAI | GPU Enthusiast  

---

ðŸ”­ Iâ€™m currently working on  
- GenAI systems at **Solytics Partners**  
- **Model Inventory**, **LLM Monitoring**, and **LLM Validation** modules  
- **Nimbus UNO** (private) â€” ML systems architecture  

ðŸŒ± Iâ€™m currently learning  
- FP8 / bfloat16 training at scale  
- Triton kernel fusion & memory-bound optimization  
- Throughput benchmarking for Transformers  

ðŸ‘¯ Iâ€™m looking to collaborate on  
- Efficient Transformer training  
- LLM evaluation & monitoring frameworks  
- Systems for high tokens/sec  

ðŸ§  Open source  
- Memory Engine for AI Assistants (Rust + PyTorch + FastAPI)  
  - Long/short-term memory graphs  
  - Retrieval + state tracking  
  - Token-efficient context building  
  - Multi-agent ready *(details undisclosed)*  

ðŸ’¬ Ask me about  
- Triton kernels  
- Quantization pipelines  
- LLM monitoring & validation  
- Throughput optimization  

ðŸ› ï¸ Tech stack  
- **Languages:** C/C++ â€¢ Python â€¢ Rust  
- **ML/Systems:** PyTorch â€¢ Triton â€¢ CUDA â€¢ Transformers â€¢ FlashAttention  
- **GenAI:** RAG â€¢ LLM eval â€¢ monitoring â€¢ validation  
- **Backend:** Django REST â€¢ FastAPI â€¢ PostgreSQL â€¢ Redis  
- **Infra:** Docker â€¢ Nginx â€¢ Linux â€¢ CI/CD â€¢ nsys â€¢ torch.profiler  

âš™ï¸ Philosophy  
Throughput > FLOPs  
Minimize memory movement  
Measure end-to-end tokens/sec  

ðŸŽ¯ Goal  
NeurIPS-level research in efficient large-scale training
